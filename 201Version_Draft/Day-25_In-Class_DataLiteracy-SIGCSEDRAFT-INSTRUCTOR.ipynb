{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 25 In-Class: Data Literacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <p style=\"text-align: right;\"> &#9989; Put your name here</p>\n",
    "#### <p style=\"text-align: right;\"> &#9989; Put your group member names here.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*rY3-IGAtdqiDDA9cutdOZA@2x.png\" width=600>\n",
    "\n",
    "## Learning Goals:\n",
    "\n",
    "By the end of this assignment, you should be able to:\n",
    "* Understand how choices made in the simulation mimic real-life inequities\n",
    "* Understand code built to create a simulation\n",
    "* Identify benefits and consequences of using ML and AI tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Survial of the Best Fit\n",
    "In the pre-class assignment, you played [\"Survival of the Best Fit\"](https://www.survivalofthebestfit.com/) and explored how bias inherent in our world and data can influence the results of an algorithm. Today, we are going to take a deeper dive in to the data that was used to create the simulation and explore how this game mimics hiring practices used by everyday companies. \n",
    "\n",
    "&#9989;&nbsp; In your groups, take a few minutes to discuss things you noticed about the game. Write down a few thoughts below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font>   *Write down some notes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "This can be led as a \"think, pair, share\" activity, where the \"think\" happened in the pre-class assignment and the \"pair\" is happening within groups. Instructors should be actively moving around the room to listen to group discussions and help facilitate deeper conversations. After the few minutes of group discussion, bring the class back together to share some of the thoughts that were discussed. One way to start this (without dealing with the awkward silence of waiting for someone to volunteer) is to ask each group to share something they discussed. Groups should be made aware of this expectation at the beginning of the activity.\n",
    "\n",
    "See the pre-class assignment for a list of possible points to bring up.\n",
    "\n",
    "If norms have not previously been established, this is a good time to do so before starting this activity. If they have been established, remind students of the norms before starting the activity. Especially since we're dealing with a sensitive topic, it's important to remind students to be respectful of each other's perspectives and lived experiences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a Deep Dive into the Code\n",
    "\n",
    "Now open the `biased_data_gen_u24.ipynb` file from the website. As a group work through that notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#9989;&nbsp; After you finish going through the biased data generation notebook, take a few minutes to reflect on the code and the data. Write down some thoughts below. How does the simulated data reflect real systemic issues present in our world?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font>   *Write down your answers here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "We can see how orange and blue people can be mapped to people of different identities in the real world. How bias is embedded within the code can also be mapped to different real world systemic issues, such as redlining, bias, and more. It is again worth emphasizing that blue and orange people are not meant to highlight that any one group is superior to the other, but rather to highlight the biases that exist in the world. Issues of bias affect the opportunities that people have access to, and it becomes reinforced when these biases are embedded in the data that is used to train algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real Life Application\n",
    "It turns out Amazon (and [many more companies](https://www.bbc.com/worklife/article/20240214-ai-recruiting-hiring-software-bias-discrimination ) more recently) have been using Machine Learning and AI algorithm to assist in their hiring practices with mixed success. \n",
    "\n",
    "&#9989;&nbsp; Take a few minutes to read [\"Amazon ditched AI recruiting tool that favored men for technical jobs\"](https://theguardian.com/technology/2018/oct/10/amazon-hiring-ai-gender-bias-recruiting-engine). \n",
    "\n",
    "### Discuss\n",
    "\n",
    "&#9989;&nbsp;  Discuss the follow questions with your group and take some notes on what you talk about. \n",
    "1.  What connections do you seen between \"Survival of the Best Fit\" and Amazon's real-life recruiting tool?\n",
    "2.  Why was Amazon's algorithm bias? What was biased about the training data? \n",
    "3.  How would you feel if you knew a company you applied to was using AI to judge candidates' applications? \n",
    "4.  If you were in a hiring role, would you feel comfortable using AI in hiring decisions? What steps might you take to help mitigate bias? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font>   *Write down some notes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANSWER\n",
    "1. Students should notice similar outcomes of bias in the results of the algorithms. \n",
    "2. Negatively impacted women as a result of more men in the tech world and hence the applicant pool for the training data. Also, bias towards language commonly used in male applications. \n",
    "3. Any reasonable answer. In past experiences, students have expressed discomfort with the idea of AI judging their applications. They've also added qualifiers, such as \"only if there was a clear explanation of how the AI was used\" or \"only if there was a human auditing the AI's decisions.\"\n",
    "4. Any reasonable answer. Might talk about reviewing the data beforehand, adding intervening stages to review applicants and algorithm decisions, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Share Your Highlights\n",
    "We will spend the last 10-15 minutes of class sharing what you learned and discussed. Make sure to note a few things to share with the rest of the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANSWER\n",
    "\n",
    "Draw from what was discussed from both looking at the code and the real-life application. This is a great time to highlight some students' insights as well as to bring up any points that were not discussed in the group (some relevant points are given in above answers, but this is certainly not meant to be limiting). This discussion serves to close out the discussion for today, so make sure to address any confusions and close out with some takeaways from below (students might not read it, so it's good to reiterate some of the main points)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways from Today's In-Class\n",
    "Today's assignment should have made you more aware about the power and the real-life harm that algorithm can cause. While they can provide great benefits like being cost-effective and speeding up time-consuming processes, they can also perpetuate harm existent in our world. Some more takeaways:\n",
    "- Pros: \n",
    "  * Algorithms can help reduce workload and synthesize large amounts of data. \n",
    "  * When implemented appropriately and thoughtfully, algorithms may help uncover patterns in candidates. \n",
    "- Cons: \n",
    "  * Algorithms reflect the data they are given. Bias in the data implies there is bias in the model. \n",
    "  * As we learned in Day 4, all data contains bias!\n",
    "  * Bias goes beyond gender, as we talked about in this case. It can also portray ageism, ableism, racism, socioeconomic status, etc. Similarly, algorithms can favor key words in resumes that might not be present in resumes of candidates that are highly qualified.\n",
    "  * Not using data that explicitly contains race, sex, or other identities does not mean that our algorithms will not negatively impact certain groups. These groups tend to be the most marginalized and underrepresented.\n",
    "\n",
    "Read More in [Algorithmic and Data Bias](https://medium.com/@sahin.samia/navigating-the-pitfalls-of-ai-in-hiring-unveiling-algorithmic-bias-9e62b50b3f65#:~:text=Imagine%20AI%20tools%20as%20mirrors,and%20diversity%20in%20the%20workplace.).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=6 color=\"#009600\">&#9998;</font>   *Add any last reflections here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Congratulations, you're done!\n",
    "\n",
    "Submit this assignment by uploading your notebook to the course Desire2Learn web page.  Go to the \"In-Class Assignments\" folder, find the appropriate submission link, and upload it there. Make sure your name is on it.\n",
    "\n",
    "See you next class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment was designed by AUTHOR XX AND AUTHOR XX (2024). \n",
    "\n",
    "&#169; Copyright 2023,  Department of Computational Mathematics, Science and Engineering at Michigan State University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
